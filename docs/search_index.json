[["index.html", "Statistics Tutorial 1 Simple Linear Regression 1.1 Intended Learning Outcomes 1.2 Simple Linear Regression", " Statistics Tutorial 1 Simple Linear Regression 1.1 Intended Learning Outcomes This tutorial will introduce the concept of modelling data using simple linear regression (SLR). In this tutorial you will use the tidyverse package. Open R Studio, create a new R Script and save this file. Now copy/paste/run the following lines of code in your R Script file in RStudio to load the library for this session: library(tidyverse) The key idea behind SLR is to attempt to explore a relationship between an: Outcome/response variable \\(y\\), and an Explanatory/predictor variable \\(x\\). Modelling generally has two main purposes: Explanation: To describe a possible relationship between an outcome variable \\(y\\) and an explanatory variable \\(x\\), and determine if the relationship statistically significant. Prediction: To be able to predict the outcome variable \\(y\\) for a new value of the explanatory variable \\(x\\). There are many different modelling techniques. In this tutorial, you will learn about one of the most commonly used approaches; linear regression. In particular, simple linear regression, where there is only one explanatory variable \\(x\\). 1.2 Simple Linear Regression A statistical model is a mathematical statement describing the variability in a random variable \\(y\\), as it relates to the explanatory variable \\(x\\). A simple linear regression model involves fitting a regression line to the data. Hence, a simple linear regression model for \\(n\\) observations of \\(x\\) and \\(y\\) can be written as follows: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] such that \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) and \\(i = 1,...,n\\), where: \\(y_i\\) is the \\(i^{th}\\) observation for the response variable. \\(\\beta_0\\) is the intercept of the regression line. \\(\\beta_1\\) is the slope of the regression line. \\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable. \\(\\epsilon_i\\) is the \\(i^{th}\\) random component. The random components \\(\\epsilon_i\\) are assumed to be normally distributed with mean 0 and constant variance \\(\\sigma^2\\). We are essentially adding random white noise to the deterministic part of the model \\(\\alpha + \\beta x_i\\) to allow for the deviations of observations from the estimated regression line. The inclusion of a random component \\(\\epsilon_i\\) makes the model statistical, rather than deterministic. The full probability model for \\(y_i\\) given \\(x_i\\) can be written as \\[y_i|x_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\] Hence the mean comes from the deterministic part of the model, and the variance comes from the random part. "],["introducing-the-data.html", "2 Introducing the Data Loading the Data 2.1 Exercise 1", " 2 Introducing the Data The movie Moneyball follows a low-budget American baseball team, the Oakland Athletics, who believed that underused statistics, such as a players ability to get on base, better predict the ability to score runs, compared to statistics typically used like home runs and batting average. Obtaining players who excelled in these underused statistics turned out to be much more affordable for the team. In this tutorial well be looking at data from all US Major League Baseball teams and examining the linear relationship between runs scored in a season and a number of other player statistics. Our aim will be to summarise these relationships both graphically and numerically in order to find which variable, if any, helps us best predict a teams runs scored in a season. The data set looks at the 2011 season in particular, and has data on all of the Major League Baseball teams, the highest paid baseball league in the United States. For this tutorial well consider 9 of the variables in the dataset. The variables we'll consider are:  team : Name of the team.  runs : Number of runs.  at_bats : Number of at bats (a batters turn hitting against a pitcher, i.e. the member of the opposite team throwing the ball at them).  hits : Number of hits.  homeruns : Number of home runs.  bat_avg : Batting average.  strikeouts : Number of strikeouts.  stolen_bases : Number of stolen bases.  wins : Number of wins. Loading the Data The data to be analyzed are saved in a .csv file called mlb11.csv which can be imported using the following code. mlb11 &lt;- read.csv(url(&quot;https://raw.githubusercontent.com/Glasgow-Stats-L1-L2/S1Z_Lab3/main/mlb11.csv&quot;)) First of all, it is always a good idea to look at the dataset you are working with to get a good sense of what it looks like, and the different types of data you may have, e.g. categorical or numerical. Recall, the dataset is called mlb11. Run the code below to look at the first 6 rows of data and the structure of it. head(mlb11) str(mlb11) How many baseball teams are there in America's Major League? 2.1 Exercise 1 Create a plot to display the relationship between runs and at_bats, i.e. with runs as your outcome/response variable and at_bats as your explanatory/predictor variable. As an added bonus, you can add + labs(title = \"\", x = \"\", y = \"\") onto your graph code to add a title and axes labels. Put the appropriate text between the quotation marks. Hint ggplot(data = ???, mapping = ???) + geom_point() + labs(???) Solution ggplot(data = mlb11, mapping = aes(x = at_bats, y = runs)) + geom_point() + labs(title = &quot;Relationship between Runs and At-Bats&quot;, x = &quot;At-Bats&quot;, y = &quot;Runs&quot;) Does the relationship between runs and at-bats look linear? If you knew a teams at-bats, would you be comfortable using a linear model to predict the number of runs? Yes, the relationship does appear approximately linear. However, we would not be comfortable using a linear model to predict the number of runs from the at-bats. We would have to consider a different varaible to choose. No, the relationship does not appear linear, and hence we would not be comfortable using a linear model to predict the number of runs from the at-bats. We would have to consider a different varaible to choose. No, the relationship does not appear linear. However, despite it's non-linearity we could still use a regression model to predict the number of runs from the at-bats. Yes, the relationship does appear approximately linear. We would therefore be comfortable using a linear model to predict the number of runs from the at-bats. If the relationship looks linear, we can quantify the strength of the relationship with the correlation coefficient. There is quite a lot of room for interpretation when looking at correlation coefficients, but a rough guide to interpreting them is given below. Correlation Coefficient Verbal Interpretation 0.9 to 1.0 or -0.9 to -1.0 Very strong positive or negative correlation 0.7 to 0.9 or -0.7 to -0.9 Strong positive or negative correlation 0.5 to 0.7 or -0.5 to -0.7 Moderate positive or negative correlation 0.3 to 0.5 or -0.3 to -0.5 Weak positive or negative correlation 0.0 to 0.3 or 0.0 to -0.3 Very weak positive or negative correlation Run the following code to find the correlation coefficient between runs and at_bats. mlb11 %&gt;% summarise(cor(runs, at_bats)) Looking at your plot and the correlation coefficient you just calculated, how would you describe the relationship between the two variables? There appears to be a strong positive linear relationship between the runs of a Major League baseball team and their at-bats. There aren't any particularly unusual observations, but the point at roughly (5525, 870) could be considered slightly unusual. There appears to be a weak positive linear relationship between the runs of a Major League baseball team and their at-bats. There aren't any unusual observations. There appears to be a moderate positive linear relationship between the runs of a Major League baseball team and their at-bats. There aren't any unusual observations There appears to be a moderate positive linear relationship between the runs of a Major League baseball team and their at-bats. There aren't any particularly unusual observations, but the point at roughly (5525, 870) could be considered slightly unusual. "],["sum-of-squared-residuals.html", "3 Sum of Squared Residuals", " 3 Sum of Squared Residuals Think back to the way that we described the distribution of a single variable. Recall that we discussed characteristics such as center, spread, and shape. Just as you've used the mean, standard deviation and other statistics to summarize a single variable, you can summarize the relationship between two variables by finding the line that best follows their association. To do this rigorously, we use something called residuals. Residuals are the difference between the observed values, (the values in the actual dataset), and the values predicted by the line/(eventually the model). Namely, \\[\\textrm{Residual} = \\textrm{Observed Value} - \\textrm{Value Fitted by the Line}\\] \\[e_i = y_i - \\hat{y_i}\\] Look at the graphs below. These show different lines, (with different gradients and intercepts), to give you an idea for what what we mean when we talk about the residual sum of squares. Black Line: The chosen regression line, that attempts to best fit the association between the two variables; in this case we still have Runs against At-Bats. Points: The observed values from the dataset Blue Line: Represents the residual of each point. Remember that the residual \\(e_i = y_i - \\hat{y_i}\\), so the blue line represents the difference between the observed value, the point, and the fitted value, the place at which the point meets the line going straight up/down towards it. Orange Squares: This represents the square of each residual. As in literally the area of each orange circle \\(= e_i^2\\). The most common way to do linear regression is to select the line that minimizes the sum of squared residuals. Literally the line that creates the smallest total area when you add up the area of all of the orange squares. You can see in the figure that some lines will be better than others, and you could probably give a reasonable guess by eye, like you did at some point in school. The process of minimising the sum of the squares of the residuals is just a way of mathematically/statistically formalising the procedure of eyeballing the best fitting line. What happens to the residuals when the line doesn't fit the points very well? Too many of the points will be too far from the line, hence the residuals will be small. In turn, the residual sum of squares, (the total area of all the orange squares), will be too large. Too many of the points will be too far from the line, hence the residuals will be too large. In turn, the residual sum of squares, (the total area of all the orange squares), will be too large. Not many of the points will be very far from the line, hence the residuals will be small. In turn, the residual sum of squares, (the total area of all the orange squares), will be small. Too many of the points will be too far from the line, hence the residuals will be too small In turn, the residual sum of squares, (the total area of all the orange squares), will be too small. "],["the-linear-model.html", "4 The Linear Model 4.1 Fitting the Linear Model 4.2 Summary of the Linear Model", " 4 The Linear Model 4.1 Fitting the Linear Model As you can imagine, doing this process of minimising the sum of the squares of the residuals through trial and error is either going to a) take far too long and/or b) be very inaccurate. This is why we use R to minimise the sums of squares of the residuals for us, and hence fit the regression line for us as well. We use another function to do this, the lm() function i.e. the linear model function. This function takes two variables, separated by a ~ with the \\(y\\) on the left and the \\(x\\) on the right, as in \\(y \\sim x\\). You also tell R where to look for the variables with the usual data = argument. For example using our runs and at_bats example we would type: m1 &lt;- lm(runs~at_bats, data = mlb11) According to the above code, we have saved the estimated regression and assigned it to the object m1 as our first model. Nothing appears when we run the code because we've not asked R to show us anything, just create the object m1. The output of this linear model function (not shown, but stored in m1) contains all of the information you would ever need about the model. To see it in a more condensed and accessible way you can use the summary function summary() on the model. 4.2 Summary of the Linear Model Run the code in the code below to look at the output of model summary. summary(m1) This looks like a daunting and unintelligible mass of information, but you will get used to seeing more of these and which parts are relevant to our needs. Let's consider the output piece by piece. First, the formula used to describe the model is shown at the top (lm(formula = runs ~ at_bats, data = mlb11)). After the formula you find the five-figure summary of the residuals. This isn't something you'll look at that often. The coefficients table shown next is key; let's just have a quick reminder of the structure of a simple linear regression model so that we know exactly what the table is telling us. Recall that for our explanatory variable \\(x\\) and our outcome variable \\(y\\) we have that: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Recall also that \\(\\beta_0\\) is the intercept of the regression line and \\(\\beta_1\\) is the slope of the regression line. Let's return to the coefficients table. Particularly important to us now is the first column, the one titled \"Estimate\". This gives us our \\(\\beta_0\\) and \\(\\beta_1\\) values. Namely the (Intercept) estimate is \\(\\beta_0\\) and the at_bats estimate is the slope \\(\\beta_1\\). So we can now write down our least squares estimated regression line as follows: \\[\\hat{y} = -2789 + 0.6305*x_i\\] \\[\\textrm{Runs} = -2789 + 0.6305*\\textrm{At-Bats}\\] One last piece of information we will discuss from the summary output is the Multiple R-squared, or \\(R^2\\). \\(R^2 =\\) The proportion of variability in the response variable that is explained by the explanatory variable Looking at our summary, we have an \\(R^2 = 0.3729\\). This means that only \\(37.3\\%\\) of the variability in the number of runs is explained by the at-bats. This is quite a low value, and implies that at-bats is not an excellent predictor of the number of runs a major league baseball team scores. 4.2.1 Exercise 2 Create a plot to display the relationship between runs and homeruns, i.e. with runs as your outcome/response variable and at_bats as your explanatory/predictor variable. Add a title and axes labels using the same method as before. Hint ggplot(data = ???, mapping = ???) + geom_point() + labs(???) Solution ggplot(data = mlb11, mapping = aes(x = homeruns, y = runs)) + geom_point() + labs(title = &quot;Relationship between Runs and Homeruns&quot;, x = &quot;Homeruns&quot;, y = &quot;Runs&quot;) Does the relationship between runs and homeruns look linear? If you knew a teams homeruns, would you be comfortable using a linear model to predict the number of runs? Yes, the relationship does appear approximately linear. However, we would not be comfortable using a linear model to predict the number of runs from the homeruns. We would have to consider a different varaible to choose. No, the relationship does not appear linear, and hence we would not be comfortable using a linear model to predict the number of runs from the homeruns. We would have to consider a different varaible to choose. Yes, the relationship does appear approximately linear. We would therefore be comfortable using a linear model to predict the number of runs from the homeruns No, the relationship does not appear linear. However, despite it's non-linearity we could still use a regression model to predict the number of runs from the homeruns. 4.2.2 Exercise 3 Fit a new model m2 that uses homeruns to predict runs, and show a summary of your new model. Hint m2 &lt;- lm(???) summary(???) Solution m2 &lt;- lm(runs~homeruns, data = mlb11) summary(m2) The equation of the regression line is A: \\(\\textrm{Homeruns} = 1.83 + 415*\\textrm{Runs}\\) B :\\(\\textrm{Runs} = 1.83 + 415*\\textrm{Homeruns}\\) C: \\(\\textrm{Runs} = 415 + 1.83*\\textrm{Homeruns}\\) D: \\(\\textrm{Homeruns} = 415 + 1.83*\\textrm{Runs}\\) What does the slope tell us in the context of the relationship between success of a team and its home runs? For a unit increase in homeruns, i.e. for each additional homerun a major league baseball team hits in the season, the expected number of runs they hit in that season increases by 1.83. For a unit increase in homeruns, i.e. for each additional homerun a major league baseball team hits in the season, the expected number of runs they hit in that season increases by 415 For a unit increase in runs, i.e. for each additional run a major league baseball team hits in the season, the expected number of homeruns they hit in that season increases by 1.83. For a unit increase in runs, i.e. for each additional run a major league baseball team hits in the season, the expected number of homeruns they hit in that season increases by 415 "],["prediction-and-prediction-errors.html", "5 Prediction and Prediction Errors 5.1 Exercise 4 5.2 Exercise 5", " 5 Prediction and Prediction Errors Let's return to looking at the relationship between runs and at_bats. We'll combine our graph before with our fitted regression line from the model superimposed on top. Recall the equation for the regression line is: \\[\\textrm{Runs} = -2789 + 0.6305*\\textrm{At-Bats}\\] ggplot(data = mlb11, aes(x = at_bats, y = runs)) + geom_point() + labs(title = &quot;Relationship between Runs and At-Bats&quot;, x = &quot;At-Bats&quot;, y = &quot;Runs&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Here, we are literally adding a layer on top of our plot. geom_smooth adds the regression line fitted by minimizing the errors sum of squares. It can also show us the standard error se associated with our line, but we won't show that for now, using the se = FALSE argument. (Although you can set se = TRUE if you're interested in looking at it. It essentially creates a continuous confidence interval for values around the regression line.) The regression line can be used to predict \\(y\\) at any value of \\(x\\) within the range of observed \\(x\\) values.. Interpolation is when... Predictions for y are made from values of x within the range of observed data. Predictions for y are made from values of x outside the range of observed data. Extrapolation is when... Predictions for y are made from values of x within the range of observed data. Predictions for y are made from values of x outside the range of observed data. Extrapolation is not usually recommended. Predictions made within the range of the data are more reliable, and are also used to compute the residuals. Use the fitted model to answer the questions below. 5.1 Exercise 4 To the nearest whole number, the New York Yankees had 867 runs and 5518 at-bats in the 2011 season. The residual for the New York Yankees is Based on your previous answer, is the fitted value for the New York Yankees an overestimate or underestimate of their runs? An overestimate. The negative residual implies the observed value is larger than the fitted value i.e. the value given by the regression line has overestimated the actual value. An overestimate. The positive residual implies the fitted value is larger than the observed value i.e. the value given by the regression line has overestimated the actual value. An underestimate. The negative residual implies the fitted value is larger than the observed value i.e. the value given by the regression line has underestimated the actual value. An underestimate. The positive residual implies the observed value is larger than the fitted value i.e. the value given by the regression line has underestimated the actual value. 5.2 Exercise 5 To the nearest whole number. the Chicago Cubs had 654 runs and 5549 at-bats in the 2011 season. The residual for the Chicago Cubs is (to 2 decimal places) Based on your previous answer, is the fitted value for the Chicago Cubs an overestimate or underestimate of their runs? An underestimate. The negative residual implies the observed value is smaller than the fitted value i.e. the value given by the regression line has underestimated the actual value. An underestimate. The negative residual implies the observed value is larger than the fitted value i.e. the value given by the regression line has underestimated the actual value. An overestimate. The negative residual implies the observed value is smaller than the fitted value i.e. the value given by the regression line has overestimated the actual value. An overestimate. The positive residual implies the observed value is larger than the fitted value i.e. the value given by the regression line has overestimated the actual value. "],["model-diagnostics.html", "6 Model Diagnostics 6.1 Assumptions of the Linear Model 6.2 How to check the assumptions of a Linear Model", " 6 Model Diagnostics Recall once more the notation for our linear model: For \\(n\\) observations of \\(x\\) and \\(y\\) we have: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) and \\(i = 1,...,n\\). The full probability model for \\(y_i\\) given \\(x_i\\) can be written as \\[y_i|x_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\] 6.1 Assumptions of the Linear Model When we fit a simple linear regression model there are five main assumptions that have to hold in order for the model to be legitimate. These assumptions are: Assumption 1: The deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero. Assumption 2: The scale of the variability of the residuals is constant at all values of the explanatory variables. Assumption 3: The residuals are normally distributed. Assumption 4: The residuals are independent. Assumption 5: The values of the explanatory variables are recorded without error. 6.2 How to check the assumptions of a Linear Model Assumption 1: Plot the residuals against the explanatory variable (e.g. at_bats), or plot the residuals against the fitted values (\\(\\hat{y}\\)). If the first assumption holds, i.e. the residuals have mean zero, then the residuals should be scattered randomly above and below the \\(y=0\\) line. Assumption 2: Plot the residuals against the fitted values. If the second assumption holds, i.e. the scale of the variability of the residuals is constant at all values of the explanatory variables, then the residuals should be scattered randomly across the \\(y=0\\) line, with no fanning or any other shape, such as a parabola. Assumption 3: Plot a histogram of the residuals, or make a QQ-plot. If the third assumption holds, i.e. the residuals are normally distributed, the histogram should appear normally distributed, in the usual bell shape and centred at zero. For the QQ-plot, the points should be as close to a straight diagonal line as possible. Assumption 4: Difficult to formally examine and often justified on the basis of experimental context. Assumption 5: Difficult to formally examine and often justified on the basis of experimental context. 6.2.1 Exercise 6 For the linear model of runs against at-bats model, does the assumption that the deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero, hold? Run the code below to look at the scatter plot of the model residuals on the y-axis against the fitted values on the x-axis. ggplot(data = m1, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) In the above code, geom_hline() adds a horizontal dashed line at y equals zero. Does the assumption hold? No, since the residuals aren't scattered evenly above and below the zero line, indicating that the residuals do not have mean zero. Yes, since the residuals aren't scattered evenly above and below the zero line, indicating that the residuals have non-zero mean. Yes, since the residuals are scattered evenly above and below the zero line, indicating that the residuals have mean zero. No, since the residuals are scattered evenly above and below the zero line, indicating that the residuals have mean zero. 6.2.2 Exercise 7 Using the same figure above, for the runs against at-bats model, does the assumption that the scale of the variability of the residuals is constant at all values of the explanatory variables, hold? No, since the residuals aren't scattered randomly across the zero line, there appears to be a slightly parabolic pattern. No, since the residuals are scattered randomly across the zero line, with no apparent pattern. Yes, since the residuals are scattered randomly across the zero line, with no apparent pattern. Yes, since the residuals aren't scattered randomly across the zero line, there appears to be a slightly parabolic pattern. See below for some examples of the second assumption not holding. Exercise 8 For the runs against at-bats model, does the assumption that the residuals are normally distributed, hold? Run the code below to look at both the histogram and the normal Q-Q plot of the model residuals. ggplot(data = m1, aes(x = .resid)) + geom_histogram(color = &quot;white&quot;, binwidth = 40) + xlab(&quot;Residuals&quot;) ggplot(data = m1, aes(sample = .resid)) + stat_qq() + stat_qq_line() Does the assumption hold? Yes, since the histogram looks bell shaped and centred at zero and the points fit near-perfectly onto the line, we have shown that the residuals are normally distributed. No, since the histogram looks bell shaped and centred at zero and the points fit near-perfectly onto the line, we have shown that the residuals are normally distributed. No, since the histogram looks more bi-modal than bell shaped and many of the QQ-plot points are very far from the line. Yes, while neither the histogram nor the QQ-plot are perfect, they both look adequate to show the residuals are approximately normally distributed. "],["bringing-it-all-together.html", "7 Bringing it all together 7.1 Exercise 9 7.2 Exercise 10 7.3 Exercise 11 7.4 Exercise 12 7.5 Exercise 13", " 7 Bringing it all together 7.1 Exercise 9 Let's look at one of the three newer variables from mlb11, slugging proportion, called new_slug. Slugging proportion is essentially a measure of the productivity of a batter. This is one of the statistics used by the author of Moneyball. Produce a scatterplot of this variable against runs. Does there seem to be a linear relationship? Hint ggplot(data = ???, mapping = ???) + geom_point() + labs(???) Solution ggplot(data = mlb11, mapping = aes(x = new_slug, y = runs)) + geom_point() + labs(title = &quot;Relationship between Runs and Slugging Prop.&quot;, x = &quot;Slugging Prop.&quot;, y = &quot;Runs&quot;) Does the relationship between runs and slugging proportion look linear? If you knew a teams slugging proportion, would you be comfortable using a linear model to predict the number of runs? Yes, the relationship does appear moderately positive and linear. We would therefore be comfortable using a linear model to predict the number of runs from the slugging proportion. Yes, the relationship does appear weakly positive and linear. We would therefore be comfortable using a linear model to predict the number of runs from the slugging proportion. Yes, the relationship does appear strong, positive and linear. Therefore, we would be comfortable using a linear model to predict the number of runs from the slugging proportion. No, the relationship does not appear linear, and hence we would not be comfortable using a linear model to predict the number of runs from the slugging proportion. We would have to consider a different varaible to choose. 7.2 Exercise 10 Fit a linear model m3 to this pair of variables new_slug and runs. Hint m3 &lt;- lm(???) summary(???) Solution m3 &lt;- lm(runs~new_slug, data = mlb11) summary(m3) How does the model between runs and slugging proportion compare to the at-bats and homeruns models? The new slugging model has an R^2 value of 0.897, significantly lower than the at-bats and homeruns models. Therefore slugging proportion seems to explain a higher proportion of the variability in runs, and hence better predicts runs. The new slugging model has an R^2 value of 0.897, roughly the same as the at-bats and homeruns models. Therefore slugging proportion seems to explain around the same proportion of the variability in runs, and hence predicts runs as well as the previous two models. The new slugging model has an R^2 value of 0.897, significantly higher than the at-bats and homeruns models. Therefore slugging proportion seems to explain a higher proportion of the variability in runs, and hence better predicts runs. The new slugging model has an R^2 value of 0.897, significantly lower than the at-bats and homeruns models. Therefore slugging proportion seems to explain a lower proportion of the variability in runs, and hence does a worse job of predicting runs. 7.3 Exercise 11 The equation of the regression line is A: \\(\\textrm{Slugging Prop} = -376 + 2681*\\textrm{Runs}\\) B :\\(\\textrm{Runs} = 2681 -376*\\textrm{Slugging Prop}\\) C: \\(\\textrm{Runs} = -376 + 2681*\\textrm{Slugging Prop}\\) D: \\(\\textrm{Slugging Prop} = 2681 - 376*\\textrm{Runs}\\) What does the slope tell us in the context of the relationship between the success of a team and its slugging percentage? For a unit increase in slugging prop, the expected number of runs they hit in that season decreases by 2681 For a unit increase in runs, the expected slugging prop in that season decreases by 2681 For a unit increase in slugging prop, the expected number of runs they hit in that season increases by 2681. For a unit increase in runs, the expected slugging prop in that season increases by 2681. 7.4 Exercise 12 Check the model assumptions of the new model. Do they all hold? Hint 1 ggplot(???) + geom_???() + geom_hline(???) + labs(???) ggplot(???) + geom_???(???) + xlab(???) ggplot(???) + stat_qq() + stat_qq_line() Hint 2 ggplot(???) + geom_point() + geom_hline(???) + labs(x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) ggplot(???) + geom_histogram(???) + xlab(&quot;Residuals&quot;) ggplot(???) + stat_qq() + stat_qq_line() Solution ggplot(data = m3, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) ggplot(data = m3, aes(x = .resid)) + geom_histogram(color = &quot;white&quot;, binwidth = 15) + xlab(&quot;Residuals&quot;) ggplot(data = m3, aes(sample = .resid)) + stat_qq() + stat_qq_line() Do all the model assumptions hold? The points are not randomly scattered across and above/below zero line, so constant variance and mean zero residual assumptions do not hold. The histogram still looks relatively normal and the qq-plot looks relatively straight and diagonal, so the normally distributed residual assumption seems to hold however. The final two assumptions are hard for us to verify, but we will assume they hold The points are randomly scattered across and above/below zero line, so constant variance and mean zero residual assumptions hold. The histogram still looks relatively normal and the qq-plot looks relatively straight and diagonal, so the normally distributed residual assumption seems to hold also. The final two assumptions are hard for us to verify, so we assume they don't hold. The points are not randomly scattered across and above/below zero line, so constant variance and mean zero residual assumptions do not hold. The histogram also looks non-normal and the qq-plot has too many deviations from the diagonal line, so the normally distributed residual assumption seems to fail as well. The final two assumptions are hard for us to verify, but we will assume they hold. The points are randomly scattered across and above/below zero line, so constant variance and mean zero residual assumptions hold. The histogram still looks relatively normal and the qq-plot looks relatively straight and diagonal, so the normally distributed residual assumption seems to hold also. The final two assumptions are hard for us to verify, but we will assume they hold 7.5 Exercise 13 Assuming the new model assumptions hold, of the three models considered in this lab, which model do you suggest is best at predicting runs and why? "],["extra-tasks.html", "8 Extra Tasks 8.1 Tasks", " 8 Extra Tasks This data set includes data for 39 species of mammals distributed over 13 orders. The data were used for analyzing the relationship between constitutional and ecological factors and sleeping in mammals. Two sleep variables (dreaming and non dreaming) were recorded. Variables such as life span, body weight, brain weight and gestation time were evaluated. Ecological variables such as severity of predation, safety of sleeping place and overall danger were inferred from field observations in the literature. The data to be analyzed are saved in a .csv file called mammals.csv which can be imported using the following code. mammals &lt;- read.csv(url(&quot;https://raw.githubusercontent.com/Glasgow-Stats-L1-L2/S1Z_Lab3/main/mammals.csv&quot;)) 8.1 Tasks Repeat Exercises 9-12 again (written below as 1. - 4.) with two variables of your choosing from the mammals data set. e.g. you could examine the relationship between a species' brain weight (\\(x\\)) and the total amount of sleep they get (\\(y\\)). But it's up to you, as long as you have the explanatory and response variables in a way round that makes sense and they are both numeric. You may wish to consider an appropriate transformation of the data (e.g. log()), particularly for any variables with a small number of large outliers. Produce a scatterplot of the variables. Does there seem to be a linear relationship? Fit a suitable linear model to the variables. What is the equation of the fitted regression line? Check the model assumptions. Do they all hold? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
